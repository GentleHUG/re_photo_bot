if __name__ == "__main__":
    from telebot.async_telebot import AsyncTeleBot
    from telebot import types
    import os
    import torch
    import tensorflow as tf
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    import pytorch_lightning as L

    import numpy as np
    import matplotlib.pyplot as plt
    import matplotlib
    import cv2
    matplotlib.use('agg')

    from torch.utils.data import DataLoader
    from pytorch_lightning.utilities import CombinedLoader
    from torchvision.io import read_image

    # Import custom classes
    from custom_classes import CustomTransform, CustomDataset, CycleGAN

    # Set seed for reproducibility
    _ = L.seed_everything(0, workers=True)
    print("num_workers", os.cpu_count(), "pin_memory", torch.cuda.is_available())

    from PIL import Image

    TOKEN = os.getenv("TOKEN")
    model_path = './model.pth'
    frame_path = "./frame1.png"
    def resize_frame_to_fit_photo(frame, photo):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–æ—Ç–æ
        photo_width, photo_height = photo.size

        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —Ä–∞–º–∫–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –ø—Ä–æ–∑—Ä–∞—á–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ —Ä–∞–∑–º–µ—Ä—É —Ñ–æ—Ç–æ
        # –ó–¥–µ—Å—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø—Ä–æ–∑—Ä–∞—á–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Ä–∞–º–∫–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        frame_width, frame_height = frame.size
        scale_width = photo_width / frame_width
        scale_height = photo_height / frame_height

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —Ä–∞–º–∫—É
        frame_resized = frame.resize((int(frame_width * scale_width), int(frame_height * scale_height)),
                                     Image.Resampling.LANCZOS)

        return frame_resized


    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    def frame (frame_path, photo_path, new_src):
        frame = Image.open(frame_path).convert("RGBA")
        photo = Image.open(photo_path).convert("RGBA")

        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —Ä–∞–º–∫–∏
        frame_resized = resize_frame_to_fit_photo(frame, photo)

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–π —Ä–∞–º–∫–∏
        combined = Image.new("RGBA", frame_resized.size)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ —Ñ–æ—Ç–æ –≤ —Ü–µ–Ω—Ç—Ä –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–π —Ä–∞–º–∫–∏
        frame_resized_width, frame_resized_height = frame_resized.size
        photo_width, photo_height = photo.size

        offset_x = (frame_resized_width - photo_width) // 2
        offset_y = (frame_resized_height - photo_height) // 2

        # –ù–∞–ª–æ–∂–µ–Ω–∏–µ —Ñ–æ—Ç–æ –Ω–∞ –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Å–º–µ—â–µ–Ω–∏—è
        combined.paste(photo, (offset_x, offset_y), photo)

        # –ù–∞–ª–æ–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–Ω–æ–π —Ä–∞–º–∫–∏ –Ω–∞ –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        combined.paste(frame_resized, (0, 0), frame_resized)
        new_img = combined.convert('RGB')

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        new_img.save(new_src)
    class CustomDataModule(L.LightningDataModule):
        def __init__(self, monet_dir, photo_dir, loader_config, sample_size, batch_size):
            super().__init__()
            self.loader_config = loader_config
            self.sample_size = sample_size
            self.batch_size = batch_size
            self.monet_filenames = monet_dir
            self.photo_filenames = photo_dir
            self.transform = CustomTransform()

        def setup(self, stage):
            if stage == "fit":
                self.train_monet = CustomDataset(self.monet_filenames, self.transform, stage)
                self.train_photo = CustomDataset(self.photo_filenames, self.transform, stage)

            if stage in ["fit", "test", "predict"]:
                self.valid_photo = CustomDataset(self.photo_filenames, self.transform, None)

        def train_dataloader(self):
            loader_config = {
                "shuffle": True,
                "drop_last": True,
                "batch_size": self.batch_size,
                **self.loader_config,
            }
            loader_monet = DataLoader(self.train_monet, **loader_config)
            loader_photo = DataLoader(self.train_photo, **loader_config)
            loaders = {"monet": loader_monet, "photo": loader_photo}
            return CombinedLoader(loaders, mode="max_size_cycle")

        def val_dataloader(self):
            return DataLoader(self.valid_photo, batch_size=self.sample_size, **self.loader_config)

        def test_dataloader(self):
            return self.val_dataloader()

        def predict_dataloader(self):
            return DataLoader(self.valid_photo, batch_size=self.batch_size, **self.loader_config)


    DEBUG = False

    DM_CONFIG = {
        "loader_config": {
            "num_workers": os.cpu_count(),
            "pin_memory": torch.cuda.is_available(),
        },
        "sample_size": 5,
        "batch_size": 5 if not DEBUG else 1,
    }

    # Continue defining other classes and training logic here...
    # Ensure all necessary imports and definitions are included

    # At the end of your script, start the training

    # Define model and training logic here...

    MODEL_CONFIG = {
        # the type of generator, and the number of residual blocks if ResNet generator is used
        "gen_name": "unet",  # types: 'unet', 'resnet'
        "num_resblocks": 6,
        # the number of filters in the first layer for the generators and discriminators
        "hid_channels": 64,
        # using DeepSpeed's FusedAdam (currently GPU only) is slightly faster
        "optimizer": torch.optim.Adam,
        # the learning rate and beta parameters for the Adam optimizer
        "lr": 2e-4,
        "betas": (0.5, 0.999),
        # the weights used in the identity loss and cycle loss
        "lambda_idt": 0.5,
        "lambda_cycle": (10, 10),  # (MPM direction, PMP direction)
        # the size of the buffer that stores previously generated images
        "buffer_size": 100,
        # the number of epochs for training
        "num_epochs": 26 if not DEBUG else 2,
        # the number of epochs before starting the learning rate decay
        "decay_epochs": 26 if not DEBUG else 1,
    }

    TRAIN_CONFIG = {
        "accelerator": "cpu",

        # train on 16-bit precision
        "precision": 32,

        # train on single GPU
        "devices": 1,

        # save checkpoint only for last epoch by default
        "enable_checkpointing": True,

        # disable logging for simplicity
        "logger": False,

        # the number of epochs for training (we limit the number of train/predict batches during debugging)
        "max_epochs": MODEL_CONFIG["num_epochs"],
        "limit_train_batches": 1.0 if not DEBUG else 2,
        "limit_predict_batches": 1.0 if not DEBUG else 5,

        # the maximum amount of time for training, in case we exceed run-time of 5 hours
        "max_time": {"hours": 4, "minutes": 55},

        # use a small subset of photos for validation/testing (we limit here for flexibility)
        "limit_val_batches": 1,
        "limit_test_batches": 5,

        # disable sanity check before starting the training routine
        "num_sanity_val_steps": 0,

        # the frequency to visualize the progress of adding Monet style
        "check_val_every_n_epoch": 6 if not DEBUG else 1,
    }

    model = CycleGAN(**MODEL_CONFIG)
    model.load_state_dict(torch.load(model_path))
    trainer = L.Trainer(**TRAIN_CONFIG)

    def save_img(img_tensor, name):

        img_tensor = img_tensor*0.5+0.5
        plt.matshow(img_tensor)
        plt.axis("off")
        plt.savefig(name, bbox_inches='tight', pad_inches=0)

    def re_photo(src, new_src):
        datamodule = CustomDataModule(
            monet_dir=[src], photo_dir=[src],
            **DM_CONFIG
        )
        predictions = trainer.predict(model, datamodule=datamodule)
        model.to("cpu")
        img = read_image(src)
        # Apply the transformation on the input image
        p = np.squeeze(predictions[0])

        imge = tf.image.resize(p.permute(1, 2, 0), list(img.shape)[1:])
        alpha = 0.5
        # print(np.array(img.permute(1,2,0)).astype(float), np.array(imge).shape)
        combined_image = cv2.addWeighted(np.array(img.permute(1,2,0)).astype(float), 1, np.array(imge).astype(float), 0, 0)

        save_img(imge, new_src)


    bot = AsyncTeleBot(TOKEN)

    # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    current_action = {}


    # –°—Ç–∞—Ä—Ç–æ–≤–∞—è –∫–æ–º–∞–Ω–¥–∞ /start
    @bot.message_handler(commands=['start'])
    async def start(message):
        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)
        btn1 = types.KeyboardButton("üëã –ü–æ–∑–¥–æ—Ä–æ–≤–∞—Ç—å—Å—è")
        markup.add(btn1)
        await bot.send_message(message.chat.id,f'–ü—Ä–∏–≤–µ—Ç, {message.from_user.username}!\n–≠—Ç–æ—Ç –±–æ—Ç –ø–æ–º–æ–∂–µ—Ç —Ç–µ–±–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏', reply_markup=markup)


    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    @bot.message_handler(content_types=['text'])
    async def get_text_messages(message):
        if message.text == 'üëã –ü–æ–∑–¥–æ—Ä–æ–≤–∞—Ç—å—Å—è':
            markup = types.ReplyKeyboardMarkup(resize_keyboard=True)
            btn1 = types.KeyboardButton('–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–º–∫—É')
            btn2 = types.KeyboardButton('–°–¥–µ–ª–∞—Ç—å —Ñ–æ—Ç–æ –ø–ª–µ–Ω–æ—á–Ω—ã–º')
            markup.add(btn1, btn2)
            await bot.send_message(message.from_user.id, '–í—ã–±–µ—Ä–∏, —á—Ç–æ —Ç–µ–±—è –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç', reply_markup=markup)
        elif message.text == '–°–¥–µ–ª–∞—Ç—å —Ñ–æ—Ç–æ –ø–ª–µ–Ω–æ—á–Ω—ã–º':
            current_action[message.from_user.id] = 'film_effect'
            await bot.send_message(message.from_user.id, '–û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–ª–µ–Ω–æ—á–Ω–æ–π',
                             parse_mode='Markdown')
        elif message.text == '–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–º–∫—É':
            current_action[message.from_user.id] = 'add_frame'
            await bot.send_message(message.from_user.id, '–û—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é, –∫ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–º–∫—É',
                             parse_mode='Markdown')


    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
    @bot.message_handler(content_types=['photo'])
    async def handle_photo(message):
        user_id = message.from_user.id
        if user_id in current_action:

            file_info = await bot.get_file(message.photo[-1].file_id)
            downloaded_file = await bot.download_file(file_info.file_path)
            src = './' + file_info.file_path
            src = src[:2] + src[9:]
            with open(src, 'wb') as new_file:
                new_file.write(downloaded_file)

            if current_action[user_id] == 'film_effect':
                await bot.send_message(user_id, '–ü–µ—Ä–µ–ø–ª–µ–Ω–æ—á–∏—Ç—ã–≤–∞—é')
                re_photo(src, src)
                await bot.send_photo(user_id, photo=open(src, 'rb'))
            elif current_action[user_id] == 'add_frame':
                await bot.send_message(user_id, '–ò—â—É —Å–≤–æ–π Instax')
                frame(frame_path, src, src)
                await bot.send_photo(user_id, photo=open(src, 'rb'))

            # –°–±—Ä–æ—Å —Ç–µ–∫—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
            current_action.pop(user_id)
        else:
            await bot.send_message(user_id, '–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏.')


    import asyncio

    asyncio.run(bot.infinity_polling())